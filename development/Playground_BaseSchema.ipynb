{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fc8099-aff8-4c7e-8343-8c697b6f8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import util\n",
    "from coffea.nanoevents import BaseSchema\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "from processor_mHrecoil import mHrecoil\n",
    "from coffea.dataset_tools import apply_to_fileset,max_chunks,preprocess\n",
    "import dask\n",
    "import copy\n",
    "from dask.diagnostics import ProgressBar\n",
    "pgb = ProgressBar()\n",
    "pgb.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd5b0d5-d90d-4863-b46e-deedc49e99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist\n",
    "from coffea.analysis_tools import Cutflow\n",
    "def lazy_summary(d,ntabs=1):\n",
    "    tab = '\\t'*ntabs\n",
    "    print_string ='{\\n'\n",
    "    for key,value in d.items():\n",
    "        print_string += f\"{tab}{key} : \"\n",
    "        if isinstance(value,dict):\n",
    "            print_string += lazy_summary(value, ntabs=ntabs+1)\n",
    "        elif isinstance(value, hist.hist.Hist):\n",
    "            print_string += f\"{type(value)}\\tIntegral:{value.sum()}\\n\"\n",
    "        elif isinstance(value, Cutflow):\n",
    "            print_string += f\"{type(value)}\\tInitial events:{value.result().nevcutflow[0]}\\n\"\n",
    "        else :\n",
    "            print_string += f\"{type(value)}\\n\"\n",
    "    print_string += tab+'}\\n'\n",
    "    return print_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89487163-7645-4fa3-99b2-817e82216d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfileset = {\n",
    "    'p8_ee_ZH_ecm240': {\n",
    "        'files': {\n",
    "            '../data/p8_ee_ZH_ecm240/events_082532938.root':'events',\n",
    "            '../data/p8_ee_ZH_ecm240/events_082532938a.root':'events',\n",
    "        },\n",
    "        'metadata': {\n",
    "            'dataset':'p8_ee_ZH_ecm240',\n",
    "            'generation':'Spring2021'\n",
    "        }\n",
    "    },\n",
    "    'p8_ee_ZZ_ecm240': {\n",
    "        'files': {\n",
    "            '../data/p8_ee_ZH_ecm240/events_082532938b.root':'events',\n",
    "            '../data/p8_ee_ZH_ecm240/events_082532938c.root':'events',\n",
    "        },\n",
    "        'metadata': {\n",
    "            'dataset':'p8_ee_ZZ_ecm240',\n",
    "            'generation':'Spring2021'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0ba4be-ee73-4e37-abea-5728c19f9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = {\n",
    "    'collider':'FCCee',\n",
    "    'campaign':'spring2021',\n",
    "    'detector':'IDEA',\n",
    "    'samples':['p8_ee_ZZ_ecm240','p8_ee_WW_ecm240','p8_ee_ZH_ecm240']\n",
    "}\n",
    "fraction = {\n",
    "    'p8_ee_ZZ_ecm240':0.005,\n",
    "    'p8_ee_WW_ecm240':0.5,\n",
    "    'p8_ee_ZH_ecm240':0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd7ca02-383f-4c69-bc78-5e67e0c45ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor='condor'\n",
    "maxchunks=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cf4946-8e6d-4678-953c-5a9aa53b7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_many(input_fileset,n):\n",
    "    '''\n",
    "    Split a given fileset into n almost even filesets\n",
    "    '''\n",
    "    \n",
    "    # Create an indexed fileset\n",
    "    fileset = copy.deepcopy(input_fileset)\n",
    "    index = 0\n",
    "    for dataset in input_fileset.keys():\n",
    "        for filename,treename in input_fileset[dataset]['files'].items():\n",
    "            fileset[dataset]['files'][filename] = {'treename': treename, 'index': index}\n",
    "            index += 1\n",
    "\n",
    "    # Split the array as required\n",
    "    nfiles = sum([len(fileset[dataset]['files']) for dataset in fileset.keys()])\n",
    "    if n == 0 :\n",
    "        return [input_fileset]\n",
    "    elif n > 0 and n <= index:\n",
    "        index_split = np.array_split(np.arange(nfiles),n)\n",
    "    else :\n",
    "        raise ValueError(f'Allowed values of n between 0 and {index}')\n",
    "\n",
    "    # Choose the required indices for each split\n",
    "    raw = [copy.deepcopy(input_fileset) for i in range(n)]\n",
    "    for f in range(n):\n",
    "        for dataset in fileset.keys():\n",
    "            for event in fileset[dataset]['files'].keys():\n",
    "                if not fileset[dataset]['files'][event]['index'] in index_split[f]:\n",
    "                    del raw[f][dataset]['files'][event]\n",
    "\n",
    "    #remove empty fields\n",
    "    out = copy.deepcopy(raw)\n",
    "    for f in range(n):\n",
    "        for dataset in raw[f].keys():\n",
    "            if len(raw[f][dataset]['files']) == 0 :\n",
    "                del out[f][dataset]\n",
    "\n",
    "    return out\n",
    "nparallel = 4\n",
    "# [print(lazy_summary(i)) for i in break_into_many(input_fileset=myfileset,n=nparallel)]\n",
    "fileset = break_into_many(input_fileset=myfileset,n=nparallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b832d95f-aa90-4f7a-a10c-357df341c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_python_file(dataset_runnable, maxchunks,filename, output_file, path, ):\n",
    "    s = f'''\n",
    "from coffea import util\n",
    "from coffea.nanoevents import BaseSchema\n",
    "import os\n",
    "from processor_mHrecoil import mHrecoil\n",
    "from coffea.dataset_tools import apply_to_fileset,max_chunks\n",
    "import dask\n",
    "\n",
    "dataset_runnable = {dataset_runnable}\n",
    "maxchunks = {maxchunks}\n",
    "\n",
    "to_compute = apply_to_fileset(\n",
    "            mHrecoil(),\n",
    "            max_chunks(dataset_runnable, maxchunks),\n",
    "            schemaclass=BaseSchema,\n",
    ")\n",
    "computed = dask.compute(to_compute)\n",
    "(Output,) = computed\n",
    "\n",
    "print(\"Saving the output to : \" , \"{output_file}\")\n",
    "if not os.path.exists(\"{path}\"):\n",
    "    os.makedirs(\"{path}\")\n",
    "util.save(output= Output, filename=\"{path}\"+\"{output_file}\")\n",
    "print(\"File {output_file} saved at {path}\")\n",
    "print(\"Execution completed.\")\n",
    "    \n",
    "    '''\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eaf8618-3a6a-4316-b62b-96936080ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_shell_file(filename, python_job_file):\n",
    "    s = f'''\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "export COFFEA_IMAGE=coffeateam/coffea-dask-almalinux8:2024.5.0-py3.11\n",
    "\n",
    "echo \"Coffea Image: ${{COFFEA_IMAGE}}\"\n",
    "\n",
    "EXTERNAL_BIND=${{PWD}}\n",
    "\n",
    "\n",
    "singularity exec -B ${{PWD}}:/srv -B /etc/condor -B /eos -B /afs -B /cvmfs --pwd /srv \\\n",
    "/cvmfs/unpacked.cern.ch/registry.hub.docker.com/${{COFFEA_IMAGE}} \\\n",
    "/usr/local/bin/python3 {python_job_file} -e dask\n",
    "    \n",
    "    '''\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f7f6c3-64c9-4e32-98e0-1de35b3590e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submit_file(filename, executable, output):\n",
    "    s = f'''\n",
    "universe = vanilla\n",
    "executable = {executable}\n",
    "\n",
    "should_transfer_files = IF_NEEDED\n",
    "when_to_transfer_output = ON_EXIT\n",
    "transfer_input_files= batch_runner_mHrecoil.py,processor_mHrecoil.py\n",
    "transfer_output_files= {output}\n",
    "\n",
    "output = out-{executable.strip('job_').strip('.sh')}.$(ClusterId).$(ProcId)\n",
    "error = err-{executable.strip('job_').strip('.sh')}.$(ClusterId).$(ProcId)\n",
    "log = log-{executable.strip('job_').strip('.sh')}.$(ClusterId).$(ProcId)\n",
    "\n",
    "queue 1\n",
    "    \n",
    "    '''\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398e9901-e6ff-47a1-8eb1-ba96f0e5c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing fileset before run...\n",
      "[########################################] | 100% Completed | 101.51 ms\n",
      "[########################################] | 100% Completed | 224.51 ms\n",
      "[########################################] | 100% Completed | 102.62 ms\n",
      "[########################################] | 100% Completed | 286.52 ms\n",
      "[########################################] | 100% Completed | 101.69 ms\n",
      "[########################################] | 100% Completed | 224.37 ms\n",
      "[########################################] | 100% Completed | 101.56 ms\n",
      "[########################################] | 100% Completed | 229.39 ms\n",
      "Executing with condor ...\n",
      "\tjob_0.py created\n",
      "\tjob_0.sh created\n",
      "\tsubmit_0.sh created\n",
      "\n",
      "universe = vanilla\n",
      "executable = job_0.sh\n",
      "\n",
      "should_transfer_files = IF_NEEDED\n",
      "when_to_transfer_output = ON_EXIT\n",
      "transfer_input_files= batch_runner_mHrecoil.py,processor_mHrecoil.py\n",
      "transfer_output_files= outputs\n",
      "\n",
      "output = out-0.$(ClusterId).$(ProcId)\n",
      "error = err-0.$(ClusterId).$(ProcId)\n",
      "log = log-0.$(ClusterId).$(ProcId)\n",
      "\n",
      "queue 1\n",
      "    \n",
      "    \n",
      "\tjob_1.py created\n",
      "\tjob_1.sh created\n",
      "\tsubmit_1.sh created\n",
      "\n",
      "universe = vanilla\n",
      "executable = job_1.sh\n",
      "\n",
      "should_transfer_files = IF_NEEDED\n",
      "when_to_transfer_output = ON_EXIT\n",
      "transfer_input_files= batch_runner_mHrecoil.py,processor_mHrecoil.py\n",
      "transfer_output_files= outputs\n",
      "\n",
      "output = out-1.$(ClusterId).$(ProcId)\n",
      "error = err-1.$(ClusterId).$(ProcId)\n",
      "log = log-1.$(ClusterId).$(ProcId)\n",
      "\n",
      "queue 1\n",
      "    \n",
      "    \n",
      "\tjob_2.py created\n",
      "\tjob_2.sh created\n",
      "\tsubmit_2.sh created\n",
      "\n",
      "universe = vanilla\n",
      "executable = job_2.sh\n",
      "\n",
      "should_transfer_files = IF_NEEDED\n",
      "when_to_transfer_output = ON_EXIT\n",
      "transfer_input_files= batch_runner_mHrecoil.py,processor_mHrecoil.py\n",
      "transfer_output_files= outputs\n",
      "\n",
      "output = out-2.$(ClusterId).$(ProcId)\n",
      "error = err-2.$(ClusterId).$(ProcId)\n",
      "log = log-2.$(ClusterId).$(ProcId)\n",
      "\n",
      "queue 1\n",
      "    \n",
      "    \n",
      "\tjob_3.py created\n",
      "\tjob_3.sh created\n",
      "\tsubmit_3.sh created\n",
      "\n",
      "universe = vanilla\n",
      "executable = job_3.sh\n",
      "\n",
      "should_transfer_files = IF_NEEDED\n",
      "when_to_transfer_output = ON_EXIT\n",
      "transfer_input_files= batch_runner_mHrecoil.py,processor_mHrecoil.py\n",
      "transfer_output_files= outputs\n",
      "\n",
      "output = out-3.$(ClusterId).$(ProcId)\n",
      "error = err-3.$(ClusterId).$(ProcId)\n",
      "log = log-3.$(ClusterId).$(ProcId)\n",
      "\n",
      "queue 1\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "output_file = \"mHrecoil_mumu.coffea\"\n",
    "path = 'outputs/FCCee/higgs/mH-recoil/mumu/'\n",
    "print('Preparing fileset before run...')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "dataset_runnable, dataset_updated = zip(*[preprocess(\n",
    "    fl,\n",
    "    align_clusters=False,\n",
    "    step_size=50_000,\n",
    "    files_per_batch=1,\n",
    "    skip_bad_files=True,\n",
    "    save_form=False,\n",
    ") for fl in fileset ]\n",
    "                                       )\n",
    "\n",
    "# print(dataset_runnable) # is a tuple\n",
    "#For local dask execution\n",
    "if executor == \"dask\" :\n",
    "    Output = []\n",
    "    print(\"Executing locally with dask ...\")\n",
    "    for i in range(len(dataset_runnable)):\n",
    "        print('Chunk : ',i)\n",
    "        to_compute = apply_to_fileset(\n",
    "                    mHrecoil(),\n",
    "                    max_chunks(dataset_runnable[i], maxchunks),\n",
    "                    schemaclass=BaseSchema,\n",
    "        )\n",
    "        computed = dask.compute(to_compute)\n",
    "        (Out,) = computed\n",
    "        Output.append(Out)\n",
    "        if nparallel > 1:\n",
    "            output_filename = output_file.strip('.coffea')+f'-chunk{i}'+'.coffea'\n",
    "        else:\n",
    "            output_filename = output_file\n",
    "        print(\"Saving the output to : \" , output_filename)\n",
    "        util.save(output= Out, filename=path+output_filename)\n",
    "        print(f\"File {output_filename} saved at {path}\")\n",
    "    print(\"Execution completed.\")\n",
    "\n",
    "#For condor execution\n",
    "elif executor == \"condor\" :\n",
    "    print(\"Executing with condor ...\")\n",
    "    if not os.path.exists('Batch'):\n",
    "        os.makedirs('Batch')\n",
    "    os.chdir('Batch')\n",
    "    for i in range(len(dataset_runnable)):\n",
    "        if nparallel > 1:\n",
    "            output_filename = output_file.strip('.coffea')+f'-chunk{i}'+'.coffea'\n",
    "        else:\n",
    "            output_filename = output_file\n",
    "        create_job_python_file(\n",
    "            dataset_runnable[i],\n",
    "            maxchunks,\n",
    "            f'job_{i}.py',\n",
    "            output_filename,\n",
    "            path\n",
    "        )\n",
    "        print(f'\\tjob_{i}.py created')\n",
    "        create_job_shell_file(\n",
    "            f'job_{i}.sh',\n",
    "            f'job_{i}.py'\n",
    "        )\n",
    "        print(f'\\tjob_{i}.sh created')\n",
    "        create_submit_file(\n",
    "            filename=f'submit_{i}.sh',\n",
    "            executable=f'job_{i}.sh',\n",
    "            output=path\n",
    "        )\n",
    "        print(f'\\tsubmit_{i}.sh created')\n",
    "        p = subprocess.run([\"cat\",f\"submit_{i}.sh\"], capture_output=True).stdout.decode(\"utf-8\")\n",
    "        print(p)\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f0ff94-fb6c-4c0f-923f-aafc91c965e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lhj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlhj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib64/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/usr/lib64/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib64/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lhj'"
     ]
    }
   ],
   "source": [
    "print(subprocess.run([\"lhj\", \"-l\", \"./\"], capture_output=True).stdout.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db701b1-76a3-491e-974d-ac41a3e7e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subprocess.run(['cat','submit_0.sh'], capture_output=True).stdout.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2163e15-16c1-4f09-8e21-e5fe65337a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
